Y <- 30 + e
Y
set.seed(123)  # For reproducibility
reject_null <- numeric(n_simulations)  # To store whether the null hypothesis is rejected
for (i in 1:n_simulations) {
# Simulate heteroscedastic errors with variance mean 100 and variance 50
# We will simulate variance for each observation from a normal distribution
variances <- rnorm(1, mean = 100, sd = sqrt(50))  #
variances
#Variance of each error
e <- rnorm(1, mean = 0, sd = sqrt(variances))
e
# Simulated errors with heteroscedasticity
# Generate Y values under null hypothesis (beta_1 = 0)
Y <- 30 + e
Y
# Fit the model Y ~ temperature
model <- lm(Y ~ temperature)
# Perform hypothesis test for beta_1 (test if beta_1 = 0)
p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature
# Record whether the null hypothesis is rejected (p-value < 0.05)
reject_null[i] <- (p_value < 0.05)
}
set.seed(123)  # For reproducibility
# Load data (assuming temperature values are stored in 'temperature' variable)
temperature <- data_tidy_air_quality$temperature
n <- length(temperature)  # Number of observations in temperature data
reject_null <- numeric(n_simulations)  # To store whether the null hypothesis is rejected
for (i in 1:n_sim) {
# Simulate heteroscedastic errors with mean variance 100 and variance of the variances = 50
# Generate different variances for each observation, ensuring mean is 100 and variance is 50
variances <- rnorm(n, mean = 100, sd = sqrt(50))  # Variance of each error term
e <- rnorm(n, mean = 0, sd = sqrt(abs(variances)))  # Simulated errors with heteroscedasticity
# Generate Y values under null hypothesis (beta_1 = 0)
Y <- 30 + e
# Fit the model Y ~ temperature
model <- lm(Y ~ temperature)
# Perform hypothesis test for beta_1 (test if beta_1 = 0)
p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
# Record whether the null hypothesis is rejected (p-value < 0.05)
reject_null[i] <- as.numeric(p_value < 0.05)
}
set.seed(123)  # For reproducibility
# Load data (assuming temperature values are stored in 'temperature' variable)
temperature <- data_tidy_air_quality$temperature
n <- length(temperature)  # Number of observations in temperature data
reject_null <- numeric(n_simulations)  # To store whether the null hypothesis is rejected
for (i in 1:n_simulations) {
# Simulate heteroscedastic errors with mean variance 100 and variance of the variances = 50
# Generate different variances for each observation, ensuring mean is 100 and variance is 50
variances <- rnorm(n, mean = 100, sd = sqrt(50))  # Variance of each error term
e <- rnorm(n, mean = 0, sd = sqrt(abs(variances)))  # Simulated errors with heteroscedasticity
# Generate Y values under null hypothesis (beta_1 = 0)
Y <- 30 + e
# Fit the model Y ~ temperature
model <- lm(Y ~ temperature)
# Perform hypothesis test for beta_1 (test if beta_1 = 0)
p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
# Record whether the null hypothesis is rejected (p-value < 0.05)
reject_null[i] <- as.numeric(p_value < 0.05)
}
# Calculate the Type I error rate (proportion of rejected null hypotheses)
type_1_error_rate <- mean(reject_null)
# Print the Type I error rate
type_1_error_rate
set.seed(123)  # For reproducibility
# Load data (assuming temperature values are stored in 'temperature' variable)
temperature <- data_tidy_air_quality$temperature
n <- length(temperature)  # Number of observations in temperature data
reject_null <- numeric(n_simulations)  # To store whether the null hypothesis is rejected
for (i in 1:n_simulations) {
# Simulate heteroscedastic errors with mean variance 100 and variance of the variances = 50
# Generate different variances for each observation, ensuring mean is 100 and variance is 50
variances <- rnorm(n, mean = 100, sd = sqrt(50))  # Variance of each error term
e <- rnorm(n, mean = 0, sd = sqrt(abs(variances)))  # Simulated errors with heteroscedasticity
# Generate Y values under null hypothesis (beta_1 = 0)
Y <- 30 + e
# Fit the model Y ~ temperature
model <- lm(Y ~ temperature)
# Perform hypothesis test for beta_1 (test if beta_1 = 0)
p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
# Record whether the null hypothesis is rejected (p-value < 0.05)
reject_null[i] <- as.numeric(p_value < 0.05)
}
# Calculate the Type I error rate (proportion of rejected null hypotheses)
type_1_error_rate <- (1000-sum(reject_null))/1000
# Print the Type I error rate
type_1_error_rate
set.seed(123)  # For reproducibility
# Load data (assuming temperature values are stored in 'temperature' variable)
temperature <- data_tidy_air_quality$temperature
n <- length(temperature)  # Number of observations in temperature data
n_sim <- 1000  # Number of simulations
# Function to generate autocorrelated errors
generate_ar1_errors <- function(n, rho, var_epsilon) {
sigma_u_squared <- var_epsilon * (1 - rho^2)
u <- rnorm(n, mean = 0, sd = sqrt(sigma_u_squared))
epsilon <- numeric(n)
epsilon[1] <- u[1]
for (i in 2:n) {
epsilon[i] <- rho * epsilon[i - 1] + u[i]
}
epsilon
}
rho <- 0.3  # Autocorrelation coefficient
var_epsilon <- 100  # Desired variance of errors
reject_null <- numeric(n_sim)  # To store whether the null hypothesis is rejected
for (i in 1:n_sim) {
# Generate autocorrelated errors with Corr(epsilon_i, epsilon_(i+1)) = 0.3
e <- generate_ar1_errors(n, rho, var_epsilon)
# Generate Y values under null hypothesis (beta_1 = 0)
Y <- 30 + e
# Fit the model Y ~ temperature
model <- lm(Y ~ temperature)
# Perform hypothesis test for beta_1 (test if beta_1 = 0)
p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
# Record whether the null hypothesis is rejected (p-value < 0.05)
reject_null[i] <- as.numeric(p_value < 0.05)
}
# Calculate the Type I error rate (proportion of rejected null hypotheses)
type_1_error_rate <- mean(reject_null)
# Print the Type I error rate
type_1_error_rate
set.seed(123)  # For reproducibility
# Load data (assuming temperature values are stored in 'temperature' variable)
temperature <- data_tidy_air_quality$temperature
n <- length(temperature)  # Number of observations in temperature data
n_sim <- 1000  # Number of simulations
# Function to generate autocorrelated errors
generate_ar1_errors <- function(n, rho, var_epsilon) {
sigma_u_squared <- var_epsilon * (1 - rho^2)
u <- rnorm(n, mean = 0, sd = sqrt(sigma_u_squared))
epsilon <- numeric(n)
epsilon[1] <- u[1]
for (i in 2:n) {
epsilon[i] <- rho * epsilon[i - 1] + u[i]
}
epsilon
}
rho <- 0.3  # Autocorrelation coefficient
var_epsilon <- 100  # Desired variance of errors
reject_null <- numeric(n_sim)  # To store whether the null hypothesis is rejected
for (i in 1:n_sim) {
# Generate autocorrelated errors with Corr(epsilon_i, epsilon_(i+1)) = 0.3
e <- generate_ar1_errors(n, rho, var_epsilon)
# Generate Y values under null hypothesis (beta_1 = 0)
Y <- 30 + e
# Fit the model Y ~ temperature
model <- lm(Y ~ temperature)
# Perform hypothesis test for beta_1 (test if beta_1 = 0)
p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
# Record whether the null hypothesis is rejected (p-value < 0.05)
reject_null[i] <- as.numeric(p_value < 0.05)
}
# Calculate the Type I error rate (proportion of rejected null hypotheses)
type_1_error_rate <- (1000-mean(reject_null))/1000
# Print the Type I error rate
type_1_error_rate
set.seed(123)  # For reproducibility
# Load data (assuming temperature values are stored in 'temperature' variable)
temperature <- data_tidy_air_quality$temperature
n <- length(temperature)  # Number of observations in temperature data
n_sim <- 1000  # Number of simulations
# Function to generate autocorrelated errors
generate_ar1_errors <- function(n, rho, var_epsilon) {
sigma_u_squared <- var_epsilon * (1 - rho^2)
u <- rnorm(n, mean = 0, sd = sqrt(sigma_u_squared))
epsilon <- numeric(n)
epsilon[1] <- u[1]
for (i in 2:n) {
epsilon[i] <- rho * epsilon[i - 1] + u[i]
}
epsilon
}
rho <- 0.3  # Autocorrelation coefficient
var_epsilon <- 100  # Desired variance of errors
reject_null <- numeric(n_sim)  # To store whether the null hypothesis is rejected
for (i in 1:n_sim) {
# Generate autocorrelated errors with Corr(epsilon_i, epsilon_(i+1)) = 0.3
e <- generate_ar1_errors(n, rho, var_epsilon)
# Generate Y values under null hypothesis (beta_1 = 0)
Y <- 30 + e
# Fit the model Y ~ temperature
model <- lm(Y ~ temperature)
# Perform hypothesis test for beta_1 (test if beta_1 = 0)
p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
# Record whether the null hypothesis is rejected (p-value < 0.05)
reject_null[i] <- as.numeric(p_value < 0.05)
}
# Calculate the Type I error rate (proportion of rejected null hypotheses)
type_1_error_rate <- (1000-sum(reject_null))/1000
# Print the Type I error rate
type_1_error_rate
library(tidyverse)
# Set the seed for reproducibility
set.seed(123)
# Number of simulations
n_simulations <- 1000
temperature <- data_tidy_air_quality$temperature
# Initialize a vector to store whether the null hypothesis was rejected in each simulation
reject_null <- numeric(n_simulations)
# Variance of the uniform distribution needs to be 100, so we calculated b = 17.32
a <- -17.32
b <- 17.32
# Run simulations
for (i in 1:n_simulations) {
# Generate error term e ~ Uniform(a, b)
e <- runif(length(temperature), min = a, max = b)
# Generate Y = 30 + e (since β1 = 0)
Y <- 30 + e
# Fit the linear model Y = β0 + β1 * temperature
model <- lm(Y ~ temperature)
# Perform hypothesis test on β1 (null hypothesis: β1 = 0) and extract from lm
p_value <- summary(model)$coefficients[2, 4]
# Record if the null hypothesis is rejected (α = 0.05)
reject_null[i] <- ifelse(p_value > 0.05, 1, 0)
}
# Calculate Type I error rate (proportion of times the null was incorrectly rejected)
type_1_error_rate <-(1000-sum(reject_null))/1000
# Output the Type I error rate
type_1_error_rate
library(tidyverse)
# Set the seed for reproducibility
set.seed(123)
# Number of simulations
n_simulations <- 1000
temperature <- data_tidy_air_quality$temperature
# Initialize a vector to store whether the null hypothesis was rejected in each simulation
reject_null <- numeric(n_simulations)
# Variance of the uniform distribution needs to be 100, so we calculated b = 17.32
a <- -17.32
b <- 17.32
# Run simulations
for (i in 1:n_simulations) {
# Generate error term e ~ Uniform(a, b)
e <- runif(length(temperature), min = a, max = b)
# Generate Y = 30 + e (since β1 = 0)
Y <- 30 + e
# Fit the linear model Y = β0 + β1 * temperature
model <- lm(Y ~ temperature)
# Perform hypothesis test on β1 (null hypothesis: β1 = 0) and extract from lm
p_value <- summary(model)$coefficients[2, 4]
# Record if the null hypothesis is rejected (α = 0.05)
reject_null[i] <- ifelse(p_value < 0.05, 1, 0)
}
# Calculate Type I error rate (proportion of times the null was incorrectly rejected)
type_1_error_rate <-(1000-sum(reject_null))/1000
# Output the Type I error rate
type_1_error_rate
library(tidyverse)
# Set the seed for reproducibility
set.seed(123)
# Number of simulations
n_simulations <- 1000
temperature <- data_tidy_air_quality$temperature
# Initialize a vector to store whether the null hypothesis was rejected in each simulation
reject_null <- numeric(n_simulations)
# Variance of the uniform distribution needs to be 100, so we calculated b = 17.32
a <- -17.32
b <- 17.32
# Run simulations
for (i in 1:5) {
# Generate error term e ~ Uniform(a, b)
e <- runif(length(temperature), min = a, max = b)
# Generate Y = 30 + e (since β1 = 0)
Y <- 30 + e
# Fit the linear model Y = β0 + β1 * temperature
model <- lm(Y ~ temperature)
# Perform hypothesis test on β1 (null hypothesis: β1 = 0) and extract from lm
p_value <- summary(model)$coefficients[2, 4]
p_value
# Record if the null hypothesis is rejected (α = 0.05)
reject_null[i] <- ifelse(p_value < 0.05, 1, 0)
}
# Calculate Type I error rate (proportion of times the null was incorrectly rejected)
type_1_error_rate <-(1000-sum(reject_null))/1000
# Output the Type I error rate
type_1_error_rate
knitr::opts_chunk$set(echo = TRUE)
#| results: hide
#| warning: false
#| message: false
#| error: false
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
remotes::install_github("MiguelRodo/DataTidyRodoSTA2005S")
data("data_tidy_air_quality", package = "DataTidyRodoSTA2005S")
head(data_tidy_air_quality)
library(ggplot2)
library(tidyr)
library(kableExtra)
mean_particle <- mean(data_tidy_air_quality$particulate_matter)
sd_particle <- sd(data_tidy_air_quality$particulate_matter)
ggplot(data_tidy_air_quality, aes(x=particulate_matter))+
geom_histogram(fill="deepskyblue3", color="black", binwidth = 3, aes(y=..density..))+
stat_function(fun=dnorm, args=list(mean=mean_particle, sd=sd_particle), color="firebrick4", size=1.2)+
labs(title="Density Plot of Particulate Matter", y="Density", x="Particulate Matter")
mean_particle <- mean(data_tidy_air_quality$particulate_matter)
sd_particle <- sd(data_tidy_air_quality$particulate_matter)
ggplot(data_tidy_air_quality, aes(x=particulate_matter))+
geom_histogram(fill="deepskyblue3", color="black", binwidth = 3, aes(y=..density..))+
stat_function(fun=dnorm, args=list(mean=mean_particle, sd=sd_particle), color="firebrick4", size=1.2)+
labs(title="Density Plot of Particulate Matter", y="Density", x="Particulate Matter")
continuous_vars <- data_tidy_air_quality[, sapply(data_tidy_air_quality, is.numeric)]
pairs(continuous_vars, main = "Pairwise Scatterplots of Continuous Variables",)
data_tidy_air_quality$industrial_activity <- factor(data_tidy_air_quality$industrial_activity,
levels = c("None","Low", "Moderate", "High"))  # Adjust the levels according to your data
data_tidy_air_quality$day_of_week <- factor(data_tidy_air_quality$day_of_week,
levels = c("Monday", "Tuesday", "Wednesday",
"Thursday", "Friday", "Saturday", "Sunday"))
data_tidy_air_quality$holiday <- factor(data_tidy_air_quality$holiday,
levels = c("Yes", "No"))
categorical_vars <- names(data_tidy_air_quality)[sapply(data_tidy_air_quality, is.factor)]
for (var in categorical_vars) {
plt<- ggplot(data_tidy_air_quality, aes_string(x = var, y = "particulate_matter")) +
geom_boxplot() +
labs(title = paste("Particulate Matter vs", var),
x = var,
y = "Particulate Matter") +
theme_minimal()
print(plt)  # Print the plot
}
for (i in 1:(length(categorical_vars)-1)) {
for (j in (i+1):length(categorical_vars)) {
cat("Contingency Table for", categorical_vars[i], "and", categorical_vars[j], "\n")
print(table(data_tidy_air_quality[[categorical_vars[i]]], data_tidy_air_quality[[categorical_vars[j]]]))
cat("\n")
}
}
for (i in 1:(length(categorical_vars)-1)) {
for (j in (i+1):length(categorical_vars)) {
cat("Contingency Table for", categorical_vars[i], "and", categorical_vars[j], "\n")
print(table(data_tidy_air_quality[[categorical_vars[i]]], data_tidy_air_quality[[categorical_vars[j]]]))
cat("\n")
}
}
for (i in 1:(length(categorical_vars) - 1)) {
for (j in (i + 1):length(categorical_vars)) {
# Create the plot
p <- ggplot(data_tidy_air_quality, aes_string(x = categorical_vars[i], fill = categorical_vars[j])) +
geom_bar(position = "fill") +  # Use "fill" to make it a stacked bar chart (proportions)
labs(title = paste("Stacked Bar Chart of", categorical_vars[i], "and", categorical_vars[j]),
x = categorical_vars[i],
y = "Proportion") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Print the plot
print(p)
}
}
View(data_tidy_air_quality)
View(data_tidy_air_quality)
install.packages("vcd")
library(vcd)
# Assuming your dataset is called 'df'
# Convert variables to factors if they are not already
df$traffic_density <- as.factor(df$traffic_density)
install.packages("vcd")
library(vcd)
# Assuming your dataset is called 'df'
# Convert variables to factors if they are not already
# Create a table of the counts for the three categorical variables
df_table <- table(data_tidy_air_quality$traffic_density, data_tidy_air_quality$day_of_week, data_tidy_air_quality$holiday_status)
library(vcd)
# Generate a mosaic plot
mosaic(~ traffic_density + day_of_week,
data = data_tidy_air_quality,
shade = TRUE, # adds shading to show deviations from independence
legend = TRUE, # add legend to explain the shading
main = "Mosaic Plot of Traffic Density, Day of Week, and Holiday Status")
for (i in 1:(length(categorical_vars) - 1)) {
for (j in (i + 1):length(categorical_vars)) {
# Create the plot
p <- ggplot(data_tidy_air_quality, aes_string(x = categorical_vars[i], fill = categorical_vars[j])) +
geom_bar(position = "fill") +  # Use "fill" to make it a stacked bar chart (proportions)
labs(title = paste("Stacked Bar Chart of", categorical_vars[i], "and", categorical_vars[j]),
x = categorical_vars[i],
y = "Proportion") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Print the plot
print(p)
}
}
knitr::opts_chunk$set(echo = TRUE)
#| results: hide
#| warning: false
#| message: false
#| error: false
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
remotes::install_github("MiguelRodo/DataTidyRodoSTA2005S")
# Summary of ANOVA results
summary(aov(particulate_matter ~ industrial_activity, data = data_tidy_air_quality))
# Calculate F-statistic and p-value manually
group_means <- tapply(data_tidy_air_quality$particulate_matter, data_tidy_air_quality$industrial_activity, mean)
overall_mean <- mean(data_tidy_air_quality$particulate_matter)
# Calculate SST
SST <- sum((data_tidy_air_quality$particulate_matter - overall_mean)^2)
# Calculate SStreatment
n <- table(data_tidy_air_quality$industrial_activity)
SStreatment <- sum(n * (group_means - overall_mean)^2)
# Calculate SSerror
group_means_vector <- unlist(tapply(data_tidy_air_quality$particulate_matter, data_tidy_air_quality$industrial_activity, mean)[data_tidy_air_quality$industrial_activity])
SSerror <- sum((data_tidy_air_quality$particulate_matter - group_means_vector)^2)
# Calculate degrees of freedom
k <- length(unique(data_tidy_air_quality$industrial_activity))
N <- nrow(data)
DFtreatment <- k - 1
DFerror <- 150 - k
# Calculate Mean Squares
MStreatment <- SStreatment / DFtreatment
MSerror <- SSerror / DFerror
# Calculate F-statistic
F_statistic <- MStreatment/MSerror
# Output F-statistic
F_statistic
# Calculate p-value
p_value <- pf(F_statistic, DFtreatment, DFerror, lower.tail = FALSE)
p_value
# Calculate F-statistic and p-value manually
group_means <- tapply(data_tidy_air_quality$particulate_matter, data_tidy_air_quality$industrial_activity, mean)
overall_mean <- mean(data_tidy_air_quality$particulate_matter)
# Calculate SST
SST <- sum((data_tidy_air_quality$particulate_matter - overall_mean)^2)
# Calculate SStreatment
n <- table(data_tidy_air_quality$industrial_activity)
n
SStreatment <- sum(n * (group_means - overall_mean)^2)
# Calculate SSerror
group_means_vector <- unlist(tapply(data_tidy_air_quality$particulate_matter, data_tidy_air_quality$industrial_activity, mean)[data_tidy_air_quality$industrial_activity])
SSerror <- sum((data_tidy_air_quality$particulate_matter - group_means_vector)^2)
# Calculate degrees of freedom
k <- length(unique(data_tidy_air_quality$industrial_activity))
N <- nrow(data)
DFtreatment <- k - 1
DFerror <- 150 - k
# Calculate Mean Squares
MStreatment <- SStreatment / DFtreatment
MSerror <- SSerror / DFerror
# Calculate F-statistic
F_statistic <- MStreatment/MSerror
F_statistic
# Calculate p-value
p_value <- pf(F_statistic, DFtreatment, DFerror, lower.tail = FALSE)
p_value
knitr::opts_chunk$set(echo = TRUE)
#| results: hide
#| warning: false
#| message: false
#| error: false
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
remotes::install_github("MiguelRodo/DataTidyRodoSTA2005S")
data("data_tidy_air_quality", package = "DataTidyRodoSTA2005S")
library(ggplot2)
library(tidyr)
library(kableExtra)
mean_particle <- mean(data_tidy_air_quality$particulate_matter)
sd_particle <- sd(data_tidy_air_quality$particulate_matter)
ggplot(data_tidy_air_quality, aes(x=particulate_matter))+
geom_histogram(fill="deepskyblue3", color="black", binwidth = 3, aes(y=..density..))+
stat_function(fun=dnorm, args=list(mean=mean_particle, sd=sd_particle), color="firebrick4", size=1.2)+
labs(title="Density Plot of Particulate Matter", y="Density", x="Particulate Matter")
continuous_vars <- data_tidy_air_quality[, sapply(data_tidy_air_quality, is.numeric)]
pairs(continuous_vars, main = "Pairwise Scatterplots of Continuous Variables",)
data_tidy_air_quality$industrial_activity <- factor(data_tidy_air_quality$industrial_activity,
levels = c("None","Low", "Moderate", "High"))  # Adjust the levels according to your data
data_tidy_air_quality$day_of_week <- factor(data_tidy_air_quality$day_of_week,
levels = c("Monday", "Tuesday", "Wednesday",
"Thursday", "Friday", "Saturday", "Sunday"))
data_tidy_air_quality$holiday <- factor(data_tidy_air_quality$holiday,
levels = c("Yes", "No"))
categorical_vars <- names(data_tidy_air_quality)[sapply(data_tidy_air_quality, is.factor)]
for (var in categorical_vars) {
plt<- ggplot(data_tidy_air_quality, aes_string(x = var, y = "particulate_matter")) +
geom_boxplot() +
labs(title = paste("Particulate Matter vs", var),
x = var,
y = "Particulate Matter") +
theme_minimal()
print(plt)  # Print the plot
}
for (i in 1:(length(categorical_vars) - 1)) {
for (j in (i + 1):length(categorical_vars)) {
# Create the plot
p <- ggplot(data_tidy_air_quality, aes_string(x = categorical_vars[i], fill = categorical_vars[j])) +
geom_bar(position = "fill") +  # Use "fill" to make it a stacked bar chart (proportions)
labs(title = paste("Stacked Bar Chart of", categorical_vars[i], "and", categorical_vars[j]),
x = categorical_vars[i],
y = "Proportion") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Print the plot
print(p)
}
}
X <- cbind(1,data_tidy_air_quality$traffic_density)
Y <-data_tidy_air_quality$particulate_matter
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y
Cmat <- solve(t(X) %*% X)
k <- ncol(X)
rss <- t(Y - X %*% bhat) %*% (Y - X %*% bhat)
# Calculate s2 = RSS/(n-k)
s2 <- as.numeric((rss)/148)
s2
c_ii <- diag(Cmat)
std.error <- sqrt(s2 * c_ii)
std.error
