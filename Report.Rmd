---
# The two fields that are not in the elsevier format are:
title: STA2005S - Regression Assignment
date: "`r Sys.Date()`"
author:
  - name: Jing Yeh
    email: yhxjin001@myuct.ac.za
  - name: Saurav Sathnarayan
    email: sthsau001@myuct.ac.za
#bibliography: mybibfile.bib
#nocite: '@*'
  
#csl: https://www.zotero.org/styles/oxford-university-press-note
#fontsize: 12pt
#spacing: halfline # could also be oneline
#classoptions:
#link-citations: yes
#urlcolor: orange
#linkcolor: green
#citecolor: red
header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
  \usepackage{caption}
  \captionsetup[figure]{font=scriptsize}

output:
  pdf_document:
    
    fig_caption: true
    number_sections: true
  oup_version: 0 # 1 = 2020 CTAN OUP CLS package; 0 = 2009 OUP CLS package
  extra_dependencies: booktabs # for kable example
  rticles::oup_article:
editor_options: 
  markdown: 
    wrap: 72
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#| results: hide
#| warning: false
#| message: false
#| error: false
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
remotes::install_github("MiguelRodo/DataTidyRodoSTA2005S")
data("data_tidy_air_quality", package = "DataTidyRodoSTA2005S")

library(ggplot2)
library(tidyr)
library(kableExtra)
```

# Part One : Analysis

## Section 1: Introduction

Air pollution, particularly high levels of particulate matter (PM), is a major environmental and public health issue in South Africaâ€™s urban centers. Exposure to elevated PM levels is linked to respiratory diseases and other serious health conditions. Understanding the factors influencing PM concentrations is crucial for developing policies that improve air quality and protect public health. This analysis seeks to identify the key drivers of air pollution in South Africa's cities, focusing on how various urban, environmental, and socioeconomic factors affect particulate matter levels.

Unknown Factors to Investigate:

Traffic Density: How do varying levels of vehicle traffic contribute to PM levels in different areas?

Industrial Activity: What is the impact of industrial activity near monitoring stations on air quality?

Temperature & Humidity: How do changes in weather conditions, like temperature and humidity, influence PM concentrations?

Wind Speed: How does wind speed affect the dispersion or accumulation of particulate matter in urban areas?

Day of the Week & Public Holidays: Do patterns of human activity on weekdays, weekends, and holidays significantly influence pollution levels?

Urban Greenery: How effective are green spaces in reducing air pollution in densely populated areas?

# Objective

The goal of this analysis is to explore the relationships between PM levels and these explanatory variables. By identifying the most influential factors, we aim to inform urban planning and public health strategies that address air pollution and improve the quality of life in South African cities.


## Section 2 : Data Exploration

### Density plot

```{r pressure, echo=FALSE, warning=FALSE}
mean_particle <- mean(data_tidy_air_quality$particulate_matter)
sd_particle <- sd(data_tidy_air_quality$particulate_matter)

ggplot(data_tidy_air_quality, aes(x=particulate_matter))+
  geom_histogram(fill="deepskyblue3", color="black", binwidth = 3, aes(y=..density..))+
  stat_function(fun=dnorm, args=list(mean=mean_particle, sd=sd_particle), color="firebrick4", size=1.2)+
  labs(title="Density Plot of Particulate Matter", y="Density", x="Particulate Matter")
```

### Pairwsie Plots

```{r echo=FALSE, warning=FALSE}
continuous_vars <- data_tidy_air_quality[, sapply(data_tidy_air_quality, is.numeric)]
pairs(continuous_vars, main = "Pairwise Scatterplots of Continuous Variables",)
```

### Categorial Variable Plots

```{r echo=FALSE, warning=FALSE}
data_tidy_air_quality$industrial_activity <- factor(data_tidy_air_quality$industrial_activity, 
                                   levels = c("None","Low", "Moderate", "High"))  # Adjust the levels according to your data

data_tidy_air_quality$day_of_week <- factor(data_tidy_air_quality$day_of_week, 
                           levels = c("Monday", "Tuesday", "Wednesday", 
                                      "Thursday", "Friday", "Saturday", "Sunday"))

data_tidy_air_quality$holiday <- factor(data_tidy_air_quality$holiday, 
                           levels = c("Yes", "No"))

categorical_vars <- names(data_tidy_air_quality)[sapply(data_tidy_air_quality, is.factor)]


for (var in categorical_vars) {
  plt<- ggplot(data_tidy_air_quality, aes_string(x = var, y = "particulate_matter")) +
    geom_boxplot() +
    labs(title = paste("Particulate Matter vs", var),
         x = var,
         y = "Particulate Matter") +
    theme_minimal() 
    
  
  print(plt)  # Print the plot
}
```

### Visual Representation of Relationship between Categorial Variables
```{r echo=FALSE, message=FALSE, warning=FALSE}
for (i in 1:(length(categorical_vars) - 1)) {
  for (j in (i + 1):length(categorical_vars)) {
    # Create the plot
    p <- ggplot(data_tidy_air_quality, aes_string(x = categorical_vars[i], fill = categorical_vars[j])) +
      geom_bar(position = "fill") +  # Use "fill" to make it a stacked bar chart (proportions)
      labs(title = paste("Stacked Bar Chart of", categorical_vars[i], "and", categorical_vars[j]),
           x = categorical_vars[i],
           y = "Proportion") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    # Print the plot
    print(p)
  }
}
```

### Comments

Distribution characterisitcs:

The distribution of particulate matter levels is generally right-skewed, indicating that a small number of observations have significantly high levels of particulate matter while most observations are clustered at lower levels. The presence of outliers suggests variations in local conditions affecting air quality.

Observed Relationships

1. Traffic Density: A positive correlation exists between particulate matter levels and traffic density, suggesting that areas with higher vehicle traffic tend to experience elevated levels of particulate matter.

2. Urban Greenery: A negative trend is observed, where higher urban greenery correlates with lower particulate matter, indicating that vegetation may help mitigate air pollution.

3. Temperature and Wind Speed: No strong relationship was identified between particulate matter and temperature. However, there is a slight negative correlation with wind speed, indicating that higher wind speeds may help disperse particulate matter.

Potential Collinearity

Some potential collinearity is observed among the explanatory variables, particularly between traffic density and urban greenery. High traffic areas often have less vegetation, leading to a relationship that may confound the analysis. Additionally, temperature and wind speed may also exhibit collinearity, as changes in one could affect the other.

# Section 3
## Simple linear regression
```{r}
X <- cbind(1,data_tidy_air_quality$traffic_density)

Y <-data_tidy_air_quality$particulate_matter
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y

Cmat <- solve(t(X) %*% X)

k <- ncol(X)
rss <- t(Y - X %*% bhat) %*% (Y - X %*% bhat)
# Calculate s2 = RSS/(n-k)
s2 <- as.numeric((rss)/148)
s2

c_ii <- diag(Cmat)

std.error <- sqrt(s2 * c_ii)
std.error
```

## Hpothesis Test

```{r echo=TRUE}
# Calculate F-statistic and p-value manually
group_means <- tapply(data_tidy_air_quality$particulate_matter,
                      data_tidy_air_quality$industrial_activity, mean)
overall_mean <- mean(data_tidy_air_quality$particulate_matter)

# Calculate SST
SST <- sum((data_tidy_air_quality$particulate_matter - overall_mean)^2)

# Calculate SStreatment
n <- table(data_tidy_air_quality$industrial_activity)
SStreatment <- sum(n * (group_means - overall_mean)^2)

# Calculate SSerror
group_means_vector <- unlist(tapply(data_tidy_air_quality$particulate_matter, data_tidy_air_quality$industrial_activity, mean)
[data_tidy_air_quality$industrial_activity])
SSerror <- sum((data_tidy_air_quality$particulate_matter - group_means_vector)^2)

# Calculate degrees of freedom
k <- length(unique(data_tidy_air_quality$industrial_activity))
N <- nrow(data)
DFtreatment <- k - 1
DFerror <- 150 - k

# Calculate Mean Squares
MStreatment <- SStreatment / DFtreatment
MSerror <- SSerror / DFerror


# Calculate F-statistic
F_statistic <- MStreatment/MSerror
F_statistic

# Calculate p-value
p_value <- pf(F_statistic, DFtreatment, DFerror, lower.tail = FALSE)
p_value
```

\newpage

# Question 4
```{r, echo=FALSE}
# Question4
multi_model <- lm(particulate_matter ~ . + temperature:humidity, data=data_tidy_air_quality)
estimates <- as.data.frame(summary(multi_model)$coefficients)
confint_df <- as.data.frame(confint(multi_model))
confint_df$Estimate <- as.numeric(estimates$Estimate)
confint_rows <-row.names(confint_df)
row_orders = c(confint_rows[1:8], "temperature:humidity", confint_rows[10:length(confint_rows) - 1])
confint_df_reordered <- confint_df[row_orders,c("2.5 %", "Estimate", "97.5 %")]

confint_table <- kable(confint_df_reordered, digits = 4, align="c", caption="Confidence Interval for each Coefficients") |>
  kable_styling(font_size = 12) |>
  pack_rows(index= c("Intercept"=1, "Traffic Density"=1, "Industrial Activity"=3, "Natural Factors" = 4, "Day of Week"=6, "Holiday"=1, "Urban Greenery"=1))
confint_table
```

### Hypothesis Testing
We'd like to perform hypothesis tests on the following variables: Temperature, Humidity, Industrial Levels, and Day of Week.

We'll start by examining whether Temperature has an effect on the concentration of Particulate Matter. We'll use the following We use the following set of hypothesis:
```{r}
#\begin{align}
#$$H_0: \beta_{temp} = \beta_{hum:temp} = 0$$ 
#$$H_A: \beta_{temp} \neq 0 \text{ and } \beta_{hum:temp} \neq 0$$
#\end{align}
#\
#This can be done by comparing the restricted and un restricted model:
#$$Y_R = \beta_0 + \beta_{traffic}X + $$
```




```{r}
model_unrestricted <- lm(particulate_matter ~ . + 
                         temperature:humidity,
                         data=data_tidy_air_quality)
model_restricted <- update(model_unrestricted, .~.
                           - temperature
                           - temperature:humidity)
anova(model_unrestricted, model_restricted)
 

```
Using the anova function in R, we compare the two models with F test, yielding a P value 0.6815, suggesting that temperature doesn't have a significant effect on the concentration of particular matter.

# part 2
# Scenario A:

## Methodology and discussion of results for Scenario A
Simulation under the null hypothesis ($\beta_{1}$= 0):

We simulate the data assuming $\beta_{0}$ = 30, $\beta_{1}$= 0, and errors are uniformly distributed.
The errors will be sampled from a uniform distribution where $e \sim U(a,b)$ with the constraint that $Var(e)$ = 100

For a uniform distribution $e \sim U(a,b)$ with a variance $\sigma^2$ = 100, $Var(e) = \frac{(b-a)^2}{12}$. Solving for $a$ and $b$ we get $a = -17.32$ and $b= 17.32$

To simulate our data, we created a function 'run_simulation' which runs a simulation once. In a simulation, we used 'runif(length(temperature), min = a, max = b)'   to generate random errors. Next calculated $Y_{i}=30 + e$. We then used the lm function to fit our regression model. We extracted p values from our model, and ran ifelse statement to check whether our null hypothesis was rejected or not. Then we used the 'textit{replicate()' function to repeat each procedure 1000 times. Finally, we counted the number of null hypothesis rejected and we observed that our type 1 error rate for this scenario is 0.043

Type I error is the probability of incorrectly rejecting the null hypothesis when it is true (false positive). Under the null hypothesis, the expected Type I error rate should be equal to the chosen significance level, typically 0.05.

Under a uniform distribution, the error terms tend to be more tightly spread compared to the tails of a normal distribution (which has more extreme values due to its longer tails). This can result in:

Underestimated variability in the model, leading to more frequent rejections of the null hypothesis when it should not be rejected.
Inflated Type I error rate: The test may incorrectly reject the null hypothesis more often than expected under the nominal significance level.

```{r include=FALSE}
library(tidyverse)
# Set the seed for reproducibility
set.seed(123)

# Number of simulations
n_simulations <- 1000

temperature <- data_tidy_air_quality$temperature

# Variance of the uniform distribution needs to be 100, so we calculated b = 17.32
a <- -17.32
b <- 17.32

# Function to run a single simulation
run_simulation <- function() {
  
  # Generate error term e ~ Uniform(a, b)
  e <- runif(length(temperature), min = a, max = b)
  
  # Generate Y = 30 + e (since b1 = 0)
  Y <- 30 + e
  
  # Fit the linear model Y = b0 + b1 * temperature
  model <- lm(Y ~ temperature)
  
  # Perform hypothesis test on b1 (null hypothesis: b1 = 0) and extract p-value
  p_value <- summary(model)$coefficients[2, 4]
  
  # Return whether the null hypothesis is rejected (a = 0.05)
  return(ifelse(p_value < 0.05, 1, 0))
}

# Run all simulations using replicate 
reject_null <- replicate(n_simulations, run_simulation())

# Calculate Type I error rate (proportion of times the null was incorrectly rejected)
type_1_error_rate <- mean(reject_null)

# Output the Type I error rate
type_1_error_rate
```

# Scenario B:

## Methodology and discussion of results for Scenario B

Again we created a single function to run a single simulation. In each simulation, We simulate the error variances using the normal distribution with mean of 100 and variance of 50. We then ensure that the variances are positive by taking the absolute value of the simulated variances. Then we simulated error terms using a normal distribution with a mean of 0 and a variance  of our error_variance calculated before. This ensures that our errors have a non constant variance. We then repeated the same steps as above to obtain our type 1 error rate. Our observated type 1 error rate is 0.047.

Impact of Heteroscedasticity:
Effect on Type I Error Rate: Heteroscedasticity violates the assumption of constant variance in regression models. This leads to incorrect estimates of standard errors and, consequently, incorrect p-values. As a result, the Type I error rate will likely increase beyond the nominal 0.05 level.

Our error rate is lower than 0,05. There could be a few reasons for this:

Conservative Hypothesis Test: Our test may be too conservative. This means the test is less likely to reject the null hypothesis even when it might be warranted. It could happen if:
The variability of the errors isn't large enough to make the standard errors inaccurate in a way that increases the Type I error.

The model has adjusted in such a way that it becomes harder to reject the null hypothesis, potentially overcorrecting for the heteroscedasticity.
```{r include=FALSE}
# For reproducibility
set.seed(123)

# Number of observations in temperature data
n <- length(temperature)

# Number of simulations
n_simulations <- 1000

# Function to run a single simulation
run_simulation <- function() {
  
  # Simulate heteroscedastic errors with mean variance 100 and variance of the variances = 50
  error_variances <- rnorm(n, mean = 100, sd = sqrt(50))  # Variance of each error term
  e <- rnorm(n, mean = 0, sd = sqrt(error_variances))  # Simulated errors with heteroscedasticity
  
  # Generate Y values under null hypothesis (beta_1 = 0)
  Y <- 30 + e
  
  # Fit the model Y ~ temperature
  model <- lm(Y ~ temperature)
  
  # Perform hypothesis test for beta_1 (test if beta_1 = 0)
  p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
  
  # Return whether the null hypothesis is rejected (p-value < 0.05)
  return(as.numeric(p_value < 0.05))
}

# Run all simulations using replicate
reject_null <- replicate(n_simulations, run_simulation())

# Calculate the Type I error rate (proportion of rejected null hypotheses)
type_1_error_rate <- mean(reject_null)

# Print the Type I error rate
type_1_error_rate

```


# Scenario C:

## Methodology and discussion of results for Scenario C:
Using our function again, we replicate 1000 simulations,but this time we test the dependence of errors in our model. To generate Correlated errrors in our model we, use the provided function given to us. As above we repeated the same steps in order to obtain the proportion of the number of times our null hypothesis  was incorrectly rejected. OUr type 1 errot rate was 0.053.

Autocorrelation violates the assumption of independent errors in linear regression. This can lead to incorrect standard errors for the regression coefficients, making hypothesis tests less reliable.

Typically, autocorrelated errors inflate Type I errors if unaccounted for because the model underestimates the true error variability. However, depending on the specific pattern of autocorrelation and sample size, this effect can vary.
```{r include=FALSE}
# Set the seed for reproducibility
set.seed(123)

# Get the temperature data
temperature <- data_tidy_air_quality$temperature

n <- length(temperature)  # Number of observations in temperature data
n_sim <- 1000  # Number of simulations

rho <- 0.3  # Autocorrelation coefficient
var_epsilon <- 100  # Desired variance of errors

# Function to generate autocorrelated errors
generate_ar1_errors <- function(n, rho, var_epsilon) {
  sigma_u_squared <- var_epsilon * (1 - rho^2)
  u <- rnorm(n, mean = 0, sd = sqrt(sigma_u_squared))
  epsilon <- numeric(n)
  epsilon[1] <- u[1]
  for (i in 2:n) {
    epsilon[i] <- rho * epsilon[i - 1] + u[i]
  }
  return(epsilon)
}

# Function to run a single simulation
run_simulation <- function(n, rho, var_epsilon) {
  # Generate autocorrelated errors
  e <- generate_ar1_errors(n, rho, var_epsilon)
  
  # Generate Y values under null hypothesis (beta_1 = 0)
  Y <- 30 + e
  
  # Fit the model Y ~ temperature
  model <- lm(Y ~ temperature)
  
  # Perform hypothesis test for beta_1 (test if beta_1 = 0)
  p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
  
  # Return whether the null hypothesis is rejected (p-value < 0.05)
  return(as.numeric(p_value < 0.05))
}

# Run all simulations using replicate
reject_null <- replicate(n_sim, run_simulation(n, rho, var_epsilon))

# Calculate the Type I error rate (proportion of rejected null hypotheses)
type_1_error_rate <- mean(reject_null)

# Print the Type I error rate
type_1_error_rate
```

