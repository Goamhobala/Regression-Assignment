---
# The two fields that are not in the elsevier format are:
title: STA2005S - Regression Assignment
date: "`r Sys.Date()`"
author:
  - name: Jing Yeh
    email: yhxjin001@myuct.ac.za
  - name: Saurav Sathnarayan
    email: sthsau001@myuct.ac.za
nocite: '@*'

abstract: In this report, we explored the efficiency of 6 programming languages through the approximation of $\pi$. We found that efficiency of various programming languages can vary widely, with C and C++ being the most efficient programming languages. We also presented evidence for compiled languages having better performance than interpreted languages. Our results suggest that programmers can benefit from taking the efficiency of various programming languages into account, rather than simply opting for simplicity in the syntax of these languages .
  
keywords: Programming Languages, Efficieny, Large-Scaled Iterative Compuations 
  
#bibliography: mybibfile.bib
#nocite: '@*'
  
#csl: https://www.zotero.org/styles/oxford-university-press-note
#fontsize: 12pt
#spacing: halfline # could also be oneline
#classoptions:
#link-citations: yes
#urlcolor: orange
#linkcolor: green
#citecolor: red
header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
  \usepackage{caption}
  \captionsetup[figure]{font=scriptsize}

output:
  pdf_document:
    
    fig_caption: true
    number_sections: true
  oup_version: 0 # 1 = 2020 CTAN OUP CLS package; 0 = 2009 OUP CLS package
  extra_dependencies: booktabs # for kable example
  rticles::oup_article:
editor_options: 
  markdown: 
    wrap: 72
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#| results: hide
#| warning: false
#| message: false
#| error: false
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
remotes::install_github("MiguelRodo/DataTidyRodoSTA2005S")
data("data_tidy_air_quality", package = "DataTidyRodoSTA2005S")
head(data_tidy_air_quality)

library(ggplot2)
library(tidyr)
library(kableExtra)
```

## Part One : Analysis

# Section 1: Introduction

Air pollution, particularly high levels of particulate matter (PM), is a major environmental and public health issue in South Africaâ€™s urban centers. Exposure to elevated PM levels is linked to respiratory diseases and other serious health conditions. Understanding the factors influencing PM concentrations is crucial for developing policies that improve air quality and protect public health. This analysis seeks to identify the key drivers of air pollution in South Africa's cities, focusing on how various urban, environmental, and socioeconomic factors affect particulate matter levels.
\
Unknown Factors to Investigate:
\
Traffic Density: How do varying levels of vehicle traffic contribute to PM levels in different areas?
\
Industrial Activity: What is the impact of industrial activity near monitoring stations on air quality?
\
Temperature & Humidity: How do changes in weather conditions, like temperature and humidity, influence PM concentrations?
\
Wind Speed: How does wind speed affect the dispersion or accumulation of particulate matter in urban areas?
\
Day of the Week & Public Holidays: Do patterns of human activity on weekdays, weekends, and holidays significantly influence pollution levels?
\
Urban Greenery: How effective are green spaces in reducing air pollution in densely populated areas?

# Objective
\
The goal of this analysis is to explore the relationships between PM levels and these explanatory variables. By identifying the most influential factors, we aim to inform urban planning and public health strategies that address air pollution and improve the quality of life in South African cities.



## Section 2 : Data Exploration

density plot

```{r pressure, echo=FALSE}
```

pairwsie plots

```{r}
continuous_vars <- data_tidy_air_quality[, sapply(data_tidy_air_quality, is.numeric)]
pairs(continuous_vars, main = "Pairwise Scatterplots of Continuous Variables")
```

categorial variable plots

```{r}
data_tidy_air_quality$industrial_activity <- factor(data_tidy_air_quality$industrial_activity, 
                                   levels = c("None","Low", "Moderate", "High"))  # Adjust the levels according to your data

data_tidy_air_quality$day_of_week <- factor(data_tidy_air_quality$day_of_week, 
                           levels = c("Monday", "Tuesday", "Wednesday", 
                                      "Thursday", "Friday", "Saturday", "Sunday"))

data_tidy_air_quality$holiday <- factor(data_tidy_air_quality$holiday, 
                           levels = c("Yes", "No"))

categorical_vars <- names(data_tidy_air_quality)[sapply(data_tidy_air_quality, is.factor)]


for (var in categorical_vars) {
  plt<- ggplot(data_tidy_air_quality, aes_string(x = var, y = "particulate_matter")) +
    geom_boxplot() +
    labs(title = paste("Particulate Matter vs", var),
         x = var,
         y = "Particulate Matter") +
    theme_minimal() 
    
  
  print(plt)  # Print the plot
}
```
tabular representation of relationship between categorial variables
```{r}
for (i in 1:(length(categorical_vars)-1)) {
  for (j in (i+1):length(categorical_vars)) {
    cat("Contingency Table for", categorical_vars[i], "and", categorical_vars[j], "\n")
    print(table(data_tidy_air_quality[[categorical_vars[i]]], data_tidy_air_quality[[categorical_vars[j]]]))
    cat("\n")
  }
}
```
visual representation of relationship between categorial variables
```{r}
for (i in 1:(length(categorical_vars) - 1)) {
  for (j in (i + 1):length(categorical_vars)) {
    # Create the plot
    p <- ggplot(data_tidy_air_quality, aes_string(x = categorical_vars[i], fill = categorical_vars[j])) +
      geom_bar(position = "fill") +  # Use "fill" to make it a stacked bar chart (proportions)
      labs(title = paste("Stacked Bar Chart of", categorical_vars[i], "and", categorical_vars[j]),
           x = categorical_vars[i],
           y = "Proportion") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    # Print the plot
    print(p)
  }
}
```
comments
\
distribution characterisitcs
\
The distribution of particulate matter levels is generally right-skewed, indicating that a small number of observations have significantly high levels of particulate matter while most observations are clustered at lower levels. The presence of outliers suggests variations in local conditions affecting air quality.
\
Observed Relationships
\
1. Traffic Density: A positive correlation exists between particulate matter levels and traffic density, suggesting that areas with higher vehicle traffic tend to experience elevated levels of particulate matter.
\
2. Urban Greenery: A negative trend is observed, where higher urban greenery correlates with lower particulate matter, indicating that vegetation may help mitigate air pollution.
\
3. Temperature and Wind Speed: No strong relationship was identified between particulate matter and temperature. However, there is a slight negative correlation with wind speed, indicating that higher wind speeds may help disperse particulate matter.

Potential Collinearity
\
Some potential collinearity is observed among the explanatory variables, particularly between traffic density and urban greenery. High traffic areas often have less vegetation, leading to a relationship that may confound the analysis. Additionally, temperature and wind speed may also exhibit collinearity, as changes in one could affect the other.




# Section 3
simple linear regression
```{r}
X <- cbind(1,data_tidy_air_quality$traffic_density)

Y <-data_tidy_air_quality$particulate_matter
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y

Cmat <- solve(t(X) %*% X)

k <- ncol(X)
rss <- t(Y - X %*% bhat) %*% (Y - X %*% bhat)
# Calculate s2 = RSS/(n-k)
s2 <- as.numeric((rss)/148)
s2

c_ii <- diag(Cmat)

std.error <- sqrt(s2 * c_ii)
std.error
mod1<-lm(data_tidy_air_quality$particulate_matter ~ data_tidy_air_quality$traffic_density, data = data_tidy_air_quality)

summary(mod1)
```
hypthesis test
```{r}


# Summary of ANOVA results
summary(aov(particulate_matter ~ industrial_activity, data = data_tidy_air_quality))

# Calculate F-statistic and p-value manually
group_means <- tapply(data_tidy_air_quality$particulate_matter, data_tidy_air_quality$industrial_activity, mean)
overall_mean <- mean(data_tidy_air_quality$particulate_matter)

# Calculate SST
SST <- sum((data_tidy_air_quality$particulate_matter - overall_mean)^2)

# Calculate SSB
n <- table(data_tidy_air_quality$industrial_activity)
SStreatment <- sum(n * (group_means - overall_mean)^2)

# Calculate SSW
group_means_vector <- unlist(tapply(data_tidy_air_quality$particulate_matter, data_tidy_air_quality$industrial_activity, mean)[data_tidy_air_quality$industrial_activity])
SSerror <- sum((data_tidy_air_quality$particulate_matter - group_means_vector)^2)

# Calculate degrees of freedom
k <- length(unique(data_tidy_air_quality$industrial_activity))
N <- nrow(data)
DFtreatment <- k - 1
DFerror <- 150 - k

# Calculate Mean Squares
MStreatment <- SStreatment / DFtreatment
MSerror <- SSerror / DFerror


# Calculate F-statistic
F_statistic <- MStreatment/MSerror

# Output F-statistic
F_statistic

# Calculate p-value
p_value <- pf(F_statistic, DFtreatment, DFerror, lower.tail = FALSE)
p_value
```

\newpage

# Question 4
```{r, echo=FALSE}
# Question4
multi_model <- lm(particulate_matter ~ . + temperature:humidity, data=data_tidy_air_quality)
estimates <- as.data.frame(summary(multi_model)$coefficients)
confint_df <- as.data.frame(confint(multi_model))
confint_df$Estimate <- as.numeric(estimates$Estimate)
confint_rows <-row.names(confint_df)
row_orders = c(confint_rows[1:8], "temperature:humidity", confint_rows[10:length(confint_rows) - 1])
confint_df_reordered <- confint_df[row_orders,c("2.5 %", "Estimate", "97.5 %")]

confint_table <- kable(confint_df_reordered, digits = 4, align="c", caption="Confidence Interval for each Coefficients") |>
  kable_styling(font_size = 12) |>
  pack_rows(index= c("Intercept"=1, "Traffic Density"=1, "Industrial Activity"=3, "Natural Factors" = 4, "Day of Week"=6, "Holiday"=1, "Urban Greenery"=1))
confint_table
```

## Hypothesis Testing
We'd like to perform hypothesis tests on the following variables: Temperature, Humidity, Industrial Levels, and Day of Week.

We'll start by examining whether Temperature has an effect on the concentration of Particulate Matter. We'll use the following We use the following set of hypothesis:


\[
\begin{aligned}
&H_0: \beta_{temp} = \beta_{hum:temp} = 0 \\
&H_A: \beta_{temp} \neq 0 \text{ and } \beta_{hum:temp} \neq 0
\end{aligned}\]

\
This can be done by comparing the restricted and un restricted model:
$$Y_R = \beta_0 + \beta_{traffic}X + $$
```{r, eval=FALSE}
model_unrestricted <- lm(particulate_matter ~ . + 
                         temperature:humidity,
                         data=data_tidy_air_quality)
model_restricted <- update(model_unrestricted, .~.
                           - temperature
                           - temperature:humidity)
anova(model_restricted, model_restricted)
 
```
Using the anova function in R, we compare the two models with F test. The F test yields a P value 0.6815, suggesting that temperature doesn't have a significant effect on the concentration of particular matter.

We now test for the effect of humidity. Repeating the same procedure, we obtain a P value < 0.00001. Suggesting that it's likely that humidity has an effect on the concentration of particulate matters.
\[
\begin{aligned}
&H_0: \beta_{hum} = \beta_{hum:temp} = 0 \\
&H_A: \beta_{hum} \neq 0 \text{ and } \beta_{hum:temp} \neq 0
\end{aligned}
\]


```{r, eval=FALSE}
model_unrestricted <- lm(particulate_matter ~ . + 
                         temperature:humidity,
                         data=data_tidy_air_quality)
model_restricted <- update(model_unrestricted, .~.
                           - humidity
                           - temperature:humidity)
anova(model_restricted, model_unrestricted)
```


### Categorical Variables
For the day of week, we take Monday as the reference category and test for the following set of hypothesis, using the same procedure.
\[
\begin{aligned}
&H_0: \beta_{Tuesday} = \beta_{Wednesday} = \beta_{Thursday} = \beta_{Friday} = \beta_{Saturday} = \beta_{Sunday} = 0 \\
&H_A: \beta_{Tuesday} \neq 0 \text{ and } \beta_{Wednesday} \neq 0 \text{ and } \beta_{Thursday} \neq 0 \text{ and } \beta_{Friday} \neq 0 \text{ and } \beta_{Saturday} \neq 0 \text{ and } \beta_{Sunday} \neq 0
\end{aligned}
\]
```{r, eval=FALSE}
data_tidy_air_quality$day_of_week <- 
  relevel(factor(data_tidy_air_quality$day_of_week), ref="Monday")
model_unrestricted <- lm(particulate_matter ~ .,
                         data=data_tidy_air_quality)
model_restricted <- update(model_unrestricted, .~.
                           - day_of_week)
anova(model_restricted, model_unrestricted)
```
The P value is 0.7735, indicating that there is no evidence that supports rejecting the null hypothesis.

We do the same for industrial activity, taking No Activity as the reference category to test for the set of hypothesis:
\[
\begin{aligned}
&H_0: \beta_{Low} = \beta_{Moderate} = \beta_{High} = 0 \\
&H_A: \beta_{Low} \neq 0 \text{ and } \beta_{Moderate} \neq 0 \text{ and } \beta_{High} \neq 0 
\end{aligned}
\]
```{r, eval=FALSE}
data_tidy_air_quality$industrial_activity <-
  relevel(factor(data_tidy_air_quality$industrial_activity), ref="None")
model_unrestricted <- lm(particulate_matter ~ .,
                         data_tidy_air_quality)
model_restricted <- lm(particulate_matter ~ . - industrial_activity,
                       data_tidy_air_quality)
anova(model_restricted, model_unrestricted)
```
We obtain a P-value of 0.0707, suggesting that there's no evidence that supports rejecting the null hypothesis at 5% significance level.

## Intepretation
From the summary output and the F test for Natural Factors, we get that only Moderate Industrial Activity (p-value: 0.03), Traffic Density (p-value: 0.0155), Urban Greenery (p-value: < 0.0001), and Humidity (p-value: < 0.0001) have a significant effect (p value less than or equal to 0.05) on the concentration of particulate matters.

The average increase in concentration of particulate matter by Moderate Industrial Activity is estimated to be 6.4545 $\frac{\mu g}{m^3}$(CI95%: 0.6047, 12.3043), indicating a significant positive association between the two factors. The average increase in concentration of particulate matter caused by Traffic Density, that is one extra vehicle per hour, is estimated to be 0.0799 $\frac{\mu g}{m^3}$ (CI95%: 0.0155, 0.1444), indicating a positive association. The average decrease in concentration of particulate matter caused by a unit in the percentage of area covered by Urban Greenery is estimated to be 0.2954 $\frac{\mu g}{m^3}$ (CI95%:-0.4142, -0.1766), indicating a negative association between the two. The effect of humidity can interact with temperature, and is thus not independent of other factors. Using the anova model, we can only deduce that there is likely an effect, but we can't be certain about the confidence interval.

# part 2
## Scenario A

### Methodology
Under the null hypothesis, the response variable is not related to temperature. We have that $\beta_0 = 0$, which means that the response variable $Y = 30 + \epsilon$ where $\epsilon$ is some random noises. We violated the model assumption by making $\epsilon \sim Unif(-17.32, 17.32)$ where $Var(\epsilon) = 100$, as opposed to being normally distributed.

```{r}
library(tidyverse)

# Set the seed for reproducibility
set.seed(123)

# Number of simulations
n_simulations <- 1000

temperature <- data_tidy_air_quality$temperature

# Initialize a vector to store whether the null hypothesis was rejected in each simulation
#reject_null <- numeric(n_simulations)

reject_counter <- 0

# Variance of the uniform distribution needs to be 100, so we calculated b = 17.32
a <- -17.32
b <- 17.32

# Run simulations
for (i in 1:n_simulations) {
  
  # Generate error term e ~ Uniform(a, b)
  e <- runif(length(temperature), min = a, max = b)
  e_norm <- rnorm(length(temperature), 0, var(temperature))
  
  # Generate Y = 30 + e (since b1 = 0)
  Y <- 30 + e
  
  
  # Fit the linear model Y = b0 + b1 * temperature
  model <- lm(Y ~ temperature)
  
  # Perform hypothesis test on b1 (null hypothesis: b1 = 0) and extract from lm
  p_value <- summary(model)$coefficients[2, 4]
  p_value
  
  # Record if the null hypothesis is rejected (a = 0.05)
  #reject_null[i] <- ifelse(p_value < 0.05, 1, 0)
  if (p_value < 0.05){
    reject_counter <- reject_counter + 1
  }

  }

# Calculate Type I error rate (proportion of times the null was incorrectly rejected)
#type_1_error_rate <-(1000-sum(reject_null))/1000
type_1_error_rate <- (reject_counter)/1000

# Output the Type I error rate

type_1_error_rate
```
### Results
We obtain a type 1 error rate of 0.054, which is not too far from 0.05. Indicating that this model assumption does not significantly affect the type 1 error rate. This can be explained by central limit theorem, with the sampling distribution of e converges to a normal distribution. (For interest's sake, if we change the simulation count to 10000, the type 1 error rate becomes 0.0501, even closer to 0.05)

scenario b
```{r}
set.seed(123)  # For reproducibility

# Load data (assuming temperature values are stored in 'temperature' variable)
temperature <- data_tidy_air_quality$temperature

n <- length(temperature)  # Number of observations in temperature data


reject_null <- numeric(n_simulations)  # To store whether the null hypothesis is rejected

for (i in 1:n_simulations) {
  # Simulate heteroscedastic errors with mean variance 100 and variance of the variances = 50
  # Generate different variances for each observation, ensuring mean is 100 and variance is 50
  variances <- rnorm(n, mean = 100, sd = sqrt(50))  # Variance of each error term
  e <- rnorm(n, mean = 0, sd = sqrt(abs(variances)))  # Simulated errors with heteroscedasticity
  
  # Generate Y values under null hypothesis (beta_1 = 0)
  Y <- 30 + e
  
  # Fit the model Y ~ temperature
  model <- lm(Y ~ temperature)
  
  # Perform hypothesis test for beta_1 (test if beta_1 = 0)
  p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
  
  # Record whether the null hypothesis is rejected (p-value < 0.05)
  reject_null[i] <- as.numeric(p_value < 0.05)
}

# Calculate the Type I error rate (proportion of rejected null hypotheses)
type_1_error_rate <- (1000-sum(reject_null))/1000

# Print the Type I error rate
type_1_error_rate
```
scenario C
```{r}
set.seed(123)  # For reproducibility

# Load data (assuming temperature values are stored in 'temperature' variable)
temperature <- data_tidy_air_quality$temperature

n <- length(temperature)  # Number of observations in temperature data
n_sim <- 1000  # Number of simulations

# Function to generate autocorrelated errors
generate_ar1_errors <- function(n, rho, var_epsilon) {
  sigma_u_squared <- var_epsilon * (1 - rho^2)
  u <- rnorm(n, mean = 0, sd = sqrt(sigma_u_squared))
  epsilon <- numeric(n)
  epsilon[1] <- u[1]
  for (i in 2:n) {
    epsilon[i] <- rho * epsilon[i - 1] + u[i]
  }
  epsilon
}

rho <- 0.3  # Autocorrelation coefficient
var_epsilon <- 100  # Desired variance of errors

reject_null <- numeric(n_sim)  # To store whether the null hypothesis is rejected

for (i in 1:n_sim) {
  # Generate autocorrelated errors with Corr(epsilon_i, epsilon_(i+1)) = 0.3
  e <- generate_ar1_errors(n, rho, var_epsilon)
  
  # Generate Y values under null hypothesis (beta_1 = 0)
  Y <- 30 + e
  
  # Fit the model Y ~ temperature
  model <- lm(Y ~ temperature)
  
  # Perform hypothesis test for beta_1 (test if beta_1 = 0)
  p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
  
  # Record whether the null hypothesis is rejected (p-value < 0.05)
  reject_null[i] <- as.numeric(p_value < 0.05)
}

# Calculate the Type I error rate (proportion of rejected null hypotheses)
type_1_error_rate <- (1000-sum(reject_null))/1000

# Print the Type I error rate
type_1_error_rate
```

