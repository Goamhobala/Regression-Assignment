---
# The two fields that are not in the elsevier format are:
title: STA2005S - Regression Assignment
date: "`r Sys.Date()`"
author:
  - name: Jing Yeh
    email: yhxjin001@myuct.ac.za
  - name: Saurav Sathnarayan
    email: sthsau001@myuct.ac.za
#bibliography: mybibfile.bib
#nocite: '@*'
  
#csl: https://www.zotero.org/styles/oxford-university-press-note
#fontsize: 12pt
#spacing: halfline # could also be oneline
#classoptions:
#link-citations: yes
#urlcolor: orange
#linkcolor: green
#citecolor: red
header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
  \usepackage{caption}
  \captionsetup[figure]{font=scriptsize}

output:
  pdf_document:
    
    fig_caption: true
    number_sections: true
  oup_version: 0 # 1 = 2020 CTAN OUP CLS package; 0 = 2009 OUP CLS package
  extra_dependencies: booktabs # for kable example
  rticles::oup_article:
editor_options: 
  markdown: 
    wrap: 72
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#| results: hide
#| warning: false
#| message: false
#| error: false
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
remotes::install_github("MiguelRodo/DataTidyRodoSTA2005S")
data("data_tidy_air_quality", package = "DataTidyRodoSTA2005S")
head(data_tidy_air_quality)

library(ggplot2)
library(tidyr)
library(kableExtra)
```

## Part One : Analysis

# Section 1: Introduction

Air pollution, particularly high levels of particulate matter (PM), is a major environmental and public health issue in South Africaâ€™s urban centers. Exposure to elevated PM levels is linked to respiratory diseases and other serious health conditions. Understanding the factors influencing PM concentrations is crucial for developing policies that improve air quality and protect public health. This analysis seeks to identify the key drivers of air pollution in South Africa's cities, focusing on how various urban, environmental, and socioeconomic factors affect particulate matter levels.
\
Unknown Factors to Investigate:
\
Traffic Density: How do varying levels of vehicle traffic contribute to PM levels in different areas?
\
Industrial Activity: What is the impact of industrial activity near monitoring stations on air quality?
\
Temperature & Humidity: How do changes in weather conditions, like temperature and humidity, influence PM concentrations?
\
Wind Speed: How does wind speed affect the dispersion or accumulation of particulate matter in urban areas?
\
Day of the Week & Public Holidays: Do patterns of human activity on weekdays, weekends, and holidays significantly influence pollution levels?
\
Urban Greenery: How effective are green spaces in reducing air pollution in densely populated areas?

# Objective
\
The goal of this analysis is to explore the relationships between PM levels and these explanatory variables. By identifying the most influential factors, we aim to inform urban planning and public health strategies that address air pollution and improve the quality of life in South African cities.



## Section 2 : Data Exploration

density plot

```{r pressure, echo=FALSE}
```

pairwsie plots

```{r}
continuous_vars <- data_tidy_air_quality[, sapply(data_tidy_air_quality, is.numeric)]
pairs(continuous_vars, main = "Pairwise Scatterplots of Continuous Variables")
```

categorial variable plots

```{r}
data_tidy_air_quality$industrial_activity <- factor(data_tidy_air_quality$industrial_activity, 
                                   levels = c("None","Low", "Moderate", "High"))  # Adjust the levels according to your data

data_tidy_air_quality$day_of_week <- factor(data_tidy_air_quality$day_of_week, 
                           levels = c("Monday", "Tuesday", "Wednesday", 
                                      "Thursday", "Friday", "Saturday", "Sunday"))

data_tidy_air_quality$holiday <- factor(data_tidy_air_quality$holiday, 
                           levels = c("Yes", "No"))

categorical_vars <- names(data_tidy_air_quality)[sapply(data_tidy_air_quality, is.factor)]


for (var in categorical_vars) {
  plt<- ggplot(data_tidy_air_quality, aes_string(x = var, y = "particulate_matter")) +
    geom_boxplot() +
    labs(title = paste("Particulate Matter vs", var),
         x = var,
         y = "Particulate Matter") +
    theme_minimal() 
    
  
  print(plt)  # Print the plot
}
```
tabular representation of relationship between categorial variables
```{r}
for (i in 1:(length(categorical_vars)-1)) {
  for (j in (i+1):length(categorical_vars)) {
    cat("Contingency Table for", categorical_vars[i], "and", categorical_vars[j], "\n")
    print(table(data_tidy_air_quality[[categorical_vars[i]]], data_tidy_air_quality[[categorical_vars[j]]]))
    cat("\n")
  }
}
```
visual representation of relationship between categorial variables
```{r}
for (i in 1:(length(categorical_vars) - 1)) {
  for (j in (i + 1):length(categorical_vars)) {
    # Create the plot
    p <- ggplot(data_tidy_air_quality, aes_string(x = categorical_vars[i], fill = categorical_vars[j])) +
      geom_bar(position = "fill") +  # Use "fill" to make it a stacked bar chart (proportions)
      labs(title = paste("Stacked Bar Chart of", categorical_vars[i], "and", categorical_vars[j]),
           x = categorical_vars[i],
           y = "Proportion") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    # Print the plot
    print(p)
  }
}
```
comments
\
distribution characterisitcs
\
The distribution of particulate matter levels is generally right-skewed, indicating that a small number of observations have significantly high levels of particulate matter while most observations are clustered at lower levels. The presence of outliers suggests variations in local conditions affecting air quality.
\
Observed Relationships
\
1. Traffic Density: A positive correlation exists between particulate matter levels and traffic density, suggesting that areas with higher vehicle traffic tend to experience elevated levels of particulate matter.
\
2. Urban Greenery: A negative trend is observed, where higher urban greenery correlates with lower particulate matter, indicating that vegetation may help mitigate air pollution.
\
3. Temperature and Wind Speed: No strong relationship was identified between particulate matter and temperature. However, there is a slight negative correlation with wind speed, indicating that higher wind speeds may help disperse particulate matter.

Potential Collinearity
\
Some potential collinearity is observed among the explanatory variables, particularly between traffic density and urban greenery. High traffic areas often have less vegetation, leading to a relationship that may confound the analysis. Additionally, temperature and wind speed may also exhibit collinearity, as changes in one could affect the other.




# Section 3
simple linear regression
```{r}
X <- cbind(1,data_tidy_air_quality$traffic_density)

Y <-data_tidy_air_quality$particulate_matter
bhat <- solve(t(X) %*% X) %*% t(X) %*% Y

Cmat <- solve(t(X) %*% X)

k <- ncol(X)
rss <- t(Y - X %*% bhat) %*% (Y - X %*% bhat)
# Calculate s2 = RSS/(n-k)
s2 <- as.numeric((rss)/148)
s2

c_ii <- diag(Cmat)

std.error <- sqrt(s2 * c_ii)
std.error
mod1<-lm(data_tidy_air_quality$particulate_matter ~ data_tidy_air_quality$traffic_density, data = data_tidy_air_quality)

summary(mod1)
```
hypthesis test
```{r}


# Summary of ANOVA results
summary(aov(particulate_matter ~ industrial_activity, data = data_tidy_air_quality))

# Calculate F-statistic and p-value manually
group_means <- tapply(data_tidy_air_quality$particulate_matter, data_tidy_air_quality$industrial_activity, mean)
overall_mean <- mean(data_tidy_air_quality$particulate_matter)

# Calculate SST
SST <- sum((data_tidy_air_quality$particulate_matter - overall_mean)^2)

# Calculate SSB
n <- table(data_tidy_air_quality$industrial_activity)
SStreatment <- sum(n * (group_means - overall_mean)^2)

# Calculate SSW
group_means_vector <- unlist(tapply(data_tidy_air_quality$particulate_matter, data_tidy_air_quality$industrial_activity, mean)[data_tidy_air_quality$industrial_activity])
SSerror <- sum((data_tidy_air_quality$particulate_matter - group_means_vector)^2)

# Calculate degrees of freedom
k <- length(unique(data_tidy_air_quality$industrial_activity))
N <- nrow(data)
DFtreatment <- k - 1
DFerror <- 150 - k

# Calculate Mean Squares
MStreatment <- SStreatment / DFtreatment
MSerror <- SSerror / DFerror


# Calculate F-statistic
F_statistic <- MStreatment/MSerror

# Output F-statistic
F_statistic

# Calculate p-value
p_value <- pf(F_statistic, DFtreatment, DFerror, lower.tail = FALSE)
p_value
```

\newpage

# Question 4
```{r, echo=FALSE}
# Question4
multi_model <- lm(particulate_matter ~ . + temperature:humidity, data=data_tidy_air_quality)
estimates <- as.data.frame(summary(multi_model)$coefficients)
confint_df <- as.data.frame(confint(multi_model))
confint_df$Estimate <- as.numeric(estimates$Estimate)
confint_rows <-row.names(confint_df)
row_orders = c(confint_rows[1:8], "temperature:humidity", confint_rows[10:length(confint_rows) - 1])
confint_df_reordered <- confint_df[row_orders,c("2.5 %", "Estimate", "97.5 %")]

confint_table <- kable(confint_df_reordered, digits = 4, align="c", caption="Confidence Interval for each Coefficients") |>
  kable_styling(font_size = 12) |>
  pack_rows(index= c("Intercept"=1, "Traffic Density"=1, "Industrial Activity"=3, "Natural Factors" = 4, "Day of Week"=6, "Holiday"=1, "Urban Greenery"=1))
confint_table
```

### Hypothesis Testing
We'd like to perform hypothesis tests on the following variables: Temperature, Humidity, Industrial Levels, and Day of Week.

We'll start by examining whether Temperature has an effect on the concentration of Particulate Matter. We'll use the following We use the following set of hypothesis:
```{r}
#\begin{align}
#$$H_0: \beta_{temp} = \beta_{hum:temp} = 0$$ 
#$$H_A: \beta_{temp} \neq 0 \text{ and } \beta_{hum:temp} \neq 0$$
#\end{align}
#\
#This can be done by comparing the restricted and un restricted model:
#$$Y_R = \beta_0 + \beta_{traffic}X + $$
```




```{r}
model_unrestricted <- lm(particulate_matter ~ . + 
                         temperature:humidity,
                         data=data_tidy_air_quality)
model_restricted <- update(model_unrestricted, .~.
                           - temperature
                           - temperature:humidity)
anova(model_unrestricted, model_restricted)
 

```
Using the anova function in R, we compare the two models with F test, yielding a P value 0.6815, suggesting that temperature doesn't have a significant effect on the concentration of particular matter.



# part 2
scenario A

```{r}
library(tidyverse)

# Set the seed for reproducibility
set.seed(123)

# Number of simulations
n_simulations <- 1000

temperature <- data_tidy_air_quality$temperature

# Initialize a vector to store whether the null hypothesis was rejected in each simulation
reject_null <- numeric(n_simulations)


# Variance of the uniform distribution needs to be 100, so we calculated b = 17.32
a <- -17.32
b <- 17.32

# Run simulations
for (i in 1:n_simulations) {
  
  # Generate error term e ~ Uniform(a, b)
  e <- runif(length(temperature), min = a, max = b)
  
  # Generate Y = 30 + e (since Î²1 = 0)
  Y <- 30 + e
  
  
  # Fit the linear model Y = Î²0 + Î²1 * temperature
  model <- lm(Y ~ temperature)
  
  # Perform hypothesis test on Î²1 (null hypothesis: Î²1 = 0) and extract from lm
  p_value <- summary(model)$coefficients[2, 4]
  p_value
  
  # Record if the null hypothesis is rejected (Î± = 0.05)
  reject_null[i] <- ifelse(p_value < 0.05, 1, 0)

  }

# Calculate Type I error rate (proportion of times the null was incorrectly rejected)
type_1_error_rate <-(1000-sum(reject_null))/1000

# Output the Type I error rate

type_1_error_rate
```
Methodology for Scenario A:
Simulation under the null hypothesis ($\beta_{1}$ = 0):
We simulate the model assuming there is no effect of temperature on particulate matter (i.e., $\beta_{1}$ = 0). Hence, the model simplifies to:
\begin{align}
$$Y_{i}= 30 + e_{i} $$
\end{align}






scenario b
```{r}
set.seed(123)  # For reproducibility

# Load data (assuming temperature values are stored in 'temperature' variable)
temperature <- data_tidy_air_quality$temperature

n <- length(temperature)  # Number of observations in temperature data


reject_null <- numeric(n_simulations)  # To store whether the null hypothesis is rejected

for (i in 1:n_simulations) {
  # Simulate heteroscedastic errors with mean variance 100 and variance of the variances = 50
  # Generate different variances for each observation, ensuring mean is 100 and variance is 50
  variances <- rnorm(n, mean = 100, sd = sqrt(50))  # Variance of each error term
  e <- rnorm(n, mean = 0, sd = sqrt(abs(variances)))  # Simulated errors with heteroscedasticity
  
  # Generate Y values under null hypothesis (beta_1 = 0)
  Y <- 30 + e
  
  # Fit the model Y ~ temperature
  model <- lm(Y ~ temperature)
  
  # Perform hypothesis test for beta_1 (test if beta_1 = 0)
  p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
  
  # Record whether the null hypothesis is rejected (p-value < 0.05)
  reject_null[i] <- as.numeric(p_value < 0.05)
}

# Calculate the Type I error rate (proportion of rejected null hypotheses)
type_1_error_rate <- (1000-sum(reject_null))/1000

# Print the Type I error rate
type_1_error_rate
```
scenario C
```{r}
set.seed(123)  # For reproducibility

# Load data (assuming temperature values are stored in 'temperature' variable)
temperature <- data_tidy_air_quality$temperature

n <- length(temperature)  # Number of observations in temperature data
n_sim <- 1000  # Number of simulations

# Function to generate autocorrelated errors
generate_ar1_errors <- function(n, rho, var_epsilon) {
  sigma_u_squared <- var_epsilon * (1 - rho^2)
  u <- rnorm(n, mean = 0, sd = sqrt(sigma_u_squared))
  epsilon <- numeric(n)
  epsilon[1] <- u[1]
  for (i in 2:n) {
    epsilon[i] <- rho * epsilon[i - 1] + u[i]
  }
  epsilon
}

rho <- 0.3  # Autocorrelation coefficient
var_epsilon <- 100  # Desired variance of errors

reject_null <- numeric(n_sim)  # To store whether the null hypothesis is rejected

for (i in 1:n_sim) {
  # Generate autocorrelated errors with Corr(epsilon_i, epsilon_(i+1)) = 0.3
  e <- generate_ar1_errors(n, rho, var_epsilon)
  
  # Generate Y values under null hypothesis (beta_1 = 0)
  Y <- 30 + e
  
  # Fit the model Y ~ temperature
  model <- lm(Y ~ temperature)
  
  # Perform hypothesis test for beta_1 (test if beta_1 = 0)
  p_value <- summary(model)$coefficients[2, 4]  # Extract p-value for temperature coefficient
  
  # Record whether the null hypothesis is rejected (p-value < 0.05)
  reject_null[i] <- as.numeric(p_value < 0.05)
}

# Calculate the Type I error rate (proportion of rejected null hypotheses)
type_1_error_rate <- (1000-sum(reject_null))/1000

# Print the Type I error rate
type_1_error_rate
```

